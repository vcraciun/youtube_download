In order to understand how certain health issues can be overcome, researchers perform various experiments on zebrafish. This is possible based on the high similarity of the human genome and that of this species. These trials allow comparisons of behaviors across healthy, ill, and cured subjects. Usually, researchers discover a new drug when the behaviors of the healthy and cured subjects match or have a high similarity rate. However, researchers face challenges such as unsuitable environments for analysis instruments, time-consuming analysis procedures, and expensive technology. We designed FARM (Framework for Activity Real-time Monitoring) to assist researchers in improving their efforts for understanding their subjects' behaviors. For this, FARM detects the scenario configuration by itself (3D or 2D bidimensional) and automates the processing of video recordings. Additionally, it attempts to describe the behavior to facilitate comparing across a wide range of comportments. In this research, we propose a possible solution to translate visual movement specifications (E.g.: heatmaps, tracking, speed, position, etc.) to a set of words and we also attempt to identify some rules to measure the distance between various behaviors accross a dataset of more than 300 video recordings.